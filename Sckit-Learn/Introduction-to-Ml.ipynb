{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0673c9a7-544e-4e23-a57a-1b68dfbc5308",
   "metadata": {},
   "source": [
    "# introduction to Scikit learn(sklearn)\n",
    "\n",
    "This notebook demonstrate some of the most useful functions of the beatifull library\n",
    "\n",
    "What we're going to cover:\n",
    "0. An end-to-end Scikit-Learn workflow\n",
    "1. Getting the data ready\n",
    "2. Choose the right estimator/algorithm for our problemns\n",
    "3. Fit the model/algorithm and use it to make predicitions on our data\n",
    "4. Evaluating a model\n",
    "5. Improve a model\n",
    "6. Save and load a trained model\n",
    "7. Putting it all together!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae66dcf9-7107-47c0-ba89-a49373959659",
   "metadata": {},
   "source": [
    "# An end - to -end scikit-learn workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4bf01577-74be-4f8d-bb2b-608f73153dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0     63    1   3       145   233    1        0      150      0      2.3   \n",
       "1     37    1   2       130   250    0        1      187      0      3.5   \n",
       "2     41    0   1       130   204    0        0      172      0      1.4   \n",
       "3     56    1   1       120   236    0        1      178      0      0.8   \n",
       "4     57    0   0       120   354    0        1      163      1      0.6   \n",
       "..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
       "298   57    0   0       140   241    0        1      123      1      0.2   \n",
       "299   45    1   3       110   264    0        1      132      0      1.2   \n",
       "300   68    1   0       144   193    1        1      141      0      3.4   \n",
       "301   57    1   0       130   131    0        1      115      1      1.2   \n",
       "302   57    0   1       130   236    0        0      174      0      0.0   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "0        0   0     1       1  \n",
       "1        0   0     2       1  \n",
       "2        2   0     2       1  \n",
       "3        2   0     2       1  \n",
       "4        2   0     2       1  \n",
       "..     ...  ..   ...     ...  \n",
       "298      1   0     3       0  \n",
       "299      1   0     3       0  \n",
       "300      1   2     3       0  \n",
       "301      1   1     3       0  \n",
       "302      1   1     2       0  \n",
       "\n",
       "[303 rows x 14 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Standart imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "heart_disease = pd.read_csv(\"heart-disease.csv\")\n",
    "heart_disease\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "76a4add6-730a-4338-983a-66f348d8343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create X(features matrix)\n",
    "X = heart_disease.drop(\"target\", axis = 1) # X é tudo exteto target\n",
    "\n",
    "#Create y(labels)\n",
    "y = heart_disease[\"target\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "907e5c92-b89c-46ce-9d8a-8482a0544025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. Choose the right model and hypermparameters\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "#We'll keep the default hyperparameters\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f989230a-3f15-41b8-8aeb-3c1a00a8666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Fit the model to the training data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5f2ea027-b830-422d-9ab3-ebf772e147e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [242, 61]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[176], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train,y_test)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:345\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 345\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    346\u001b[0m     X, y, multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mDTYPE\n\u001b[0;32m    347\u001b[0m )\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1124\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1106\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m-> 1124\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [242, 61]"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train,y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc24e4a-d4a1-4630-9910-2bd48fa3f31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d3bc9-aa5c-4374-a221-df30335dfc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = clf.predict(X_test)\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854d8918-825b-4d52-97dd-42177520c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caabbed8-743d-47fd-bc80-ac7b85a47957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Evaluate the model on the training data and test data\n",
    "clf.score(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf39bac4-4299-4afa-8606-ede57a638407",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfeb9d2-bb2a-43e1-9a72-c1f027f71a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
    "print(classification_report(y_test,y_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c51f277-cc1e-44ad-a3e6-2b7459948656",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07182d94-8096-4875-bb86-b65815e95f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Improve a model\n",
    "#Try differente amount of n-estimators\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "for i in range(10,100,10):\n",
    "    print(f'Trying model with {i} estimators..')\n",
    "    clf= RandomForestClassifier(n_estimators=i).fit(X_train,y_train)\n",
    "    print(f\"Model accuracy on test set : {clf.score(X_test,y_test) * 100:.2f}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e36640-f75a-438d-be11-390ab1f87877",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save a model and load it\n",
    "import pickle\n",
    "pickle.dump(clf, open(\"Random_forest_model_1.pkl\",\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7709d1d-c3f0-437a-950a-cdd9c60e7371",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(\"Random_forest_model_1.pkl\",\"rb\"))\n",
    "loaded_model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87fd14d-03a2-451c-8893-12bc695d3672",
   "metadata": {},
   "source": [
    "## 1. Getting our data ready to be used with learning\n",
    "\n",
    "Three main thing we have to do:\n",
    "    1. Split the into features and labels(usually 'X' and 'y');\n",
    "    2. Filling (also called imputing) or disreagarding missing values;\n",
    "    3. Converting non-numerical values to numerical values(also called feature encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fcaae8-72cf-4625-800e-a5f1d03a0971",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d123b-f1db-432c-ba2c-2423ec3e6a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = heart_disease.drop(\"target\",axis = 1)\n",
    "y= heart_disease[\"target\"]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915425d3-dc0b-490f-808f-0be8938a6053",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e6370f-8d4a-414a-ac6d-ed7f4ad94ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e5b3f-35f6-4670-ba7b-dbed63532ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6adc41-7db4-47b4-b61c-fe9023e2320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[0] * 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d145a228-bd6b-4f96-9366-53a3227401bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(heart_disease)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13f30df-b61b-4345-8106-0b10bc4370b5",
   "metadata": {},
   "source": [
    "## 1.1 Make sure it's all numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893470ae-08e1-447c-8a66-3b9a3c52dc34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "car_sales = pd.read_csv('zero-to-mastery-ml-master/data/car-sales-extended.csv')\n",
    "car_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ae6547-f572-4d67-b28a-5e3d90d572e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(car_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677bd65b-a4cc-4eb5-9b12-156aacb33909",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315d7dfe-02bc-484b-a01b-50776462a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into X/y\n",
    "X = car_sales.drop(\"Price\",axis =1)\n",
    "y= car_sales[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201e5f18-87f9-4ae0-9c3f-f37d7918cfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPlit into training and test\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b739a07-a508-4552-9b8e-f9343904722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build machine learning model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76bc429-060d-47b5-8105-f5bd888ba0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Supondo que suas colunas categóricas sejam \"Make\", \"Colour\", e \"Doors\"\n",
    "categorical_features = [\"Make\", \"Colour\", \"Doors\"]\n",
    "\n",
    "# Criar o transformador para as colunas categóricas\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"encoder\", OneHotEncoder(), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # Mantém as outras colunas que não são especificadas\n",
    ")\n",
    "\n",
    "# Suponha que X é o seu DataFrame de características\n",
    "# Aplicar a transformação\n",
    "transformed_X = column_transformer.fit_transform(X)\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8037ccde-93b3-4d27-8af8-538b7d47289c",
   "metadata": {},
   "source": [
    "### Como funciona o OneHotEncoder\n",
    "-> Pegando a colunas colour por exemplo, podemos observar que ela é definida em red, green e blue.\n",
    "Em alguns modelos como o randomForest, precisamos que nossas variaveis nao sejam categóricas, logo, usamos o oneHotEncoder.\n",
    "->Quando ele fizer a sua funçao na coluna, cada cor sera um numero binario \n",
    "vermelhor 100\n",
    "verde 010\n",
    "azul 001\n",
    "Nesse caso, vamos tirar as variaveis categoricas dele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0d45a9-9862-4b9f-bdb9-959f64da6052",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(transformed_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573409e3-855c-4f41-918d-6524babe3eee",
   "metadata": {},
   "source": [
    "Dummies é uma funçao que é utilizada para converter variaveis categoricas em variaveis dummies/indicadoras. É bastante utilizada no preprocessamento de dados pois nos precisamos de entradas numericas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40222b4c-edd6-433d-a540-d09b90746b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dummies = pd.get_dummies(car_sales[[\"Make\",\"Colour\", \"Doors\"]])\n",
    "dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b0dca0-4a64-49d2-a6a6-8d5c090b627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's refit the model\n",
    "np.random.seed(42)\n",
    "X_train,X_test,y_train,y_test = train_test_split(transformed_X,y,test_size = 0.2)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b510d99-ca08-43cf-8631-63434415f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.score(X_test,y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8450e7cd-51c8-466e-a280-86fecfb68cf5",
   "metadata": {},
   "source": [
    "### 1.2 What if there were missing values?\n",
    "\n",
    "1.Fill them with some value(also know as imputation)\n",
    "2.Remove the samples with missing data altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8968028d-eaeb-44d7-b3df-4c02d973d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing car sales missing data\n",
    "car_sales_missing = pd.read_csv('zero-to-mastery-ml-master/data/car-sales-extended-missing-data.csv')\n",
    "car_sales_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c5b01d-cc11-4a9c-bc76-e40430e98e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6705aaf-705b-44cc-bede-36b2af3decf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X e Y\n",
    "X = car_sales_missing.drop(\"Price\", axis =1 )\n",
    "y = car_sales_missing[\"Price\"]\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094e573e-82b9-4de7-85a2-986f0b25ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temos que tratar os valores Nan\n",
    "car_sales_missing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0ec7d-7b38-4bd0-bb8b-4bce5e40a67b",
   "metadata": {},
   "source": [
    "#### Option 1 : FIll missing data with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ae703-0cd6-41ee-9802-df0606263271",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing[\"Doors\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9124ca92-fb59-4898-9911-efa01bc69949",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill the 'make' column\n",
    "car_sales_missing[\"Make\"].fillna(\"missing\", inplace = True)##Inplace é o comando para alterar direto no dataframe original\n",
    "#fill the 'Colour' column\n",
    "car_sales_missing[\"Colour\"].fillna('missing', inplace = True)\n",
    "#fill the 'Odometer{km)' COLUMN\n",
    "\n",
    "car_sales_missing[\"Odometer (KM)\"].fillna(car_sales_missing[\"Odometer (KM)\"].mean(),inplace = True)\n",
    "##Fills the doors column\n",
    "car_sales_missing[\"Doors\"].fillna(4,inplace = True)#Prrenchendo os dados faltantes das poras com valor 4\n",
    "\n",
    "                                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3f2a98-94ba-4942-9faa-227726bbb630",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a4ce3d-a905-4da6-96cb-ee3f4d22d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows with missing Price value\n",
    "#Dropna -> Metodo do pandar para excluir linhas ou colunas que contem valores faltantes\n",
    "car_sales_missing.dropna(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dafaf9e-54b1-483a-9633-4ba9cb2ecf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb607f57-f9f5-48f8-a754-6f51048c0a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(car_sales_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2411ac4b-0973-4594-bfab-0c5d0b9a92b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = car_sales_missing.drop(\"Price\",axis = 1)\n",
    "y = car_sales_missing[\"Price\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7479271-9ec7-4cab-a45e-c387616140c6",
   "metadata": {},
   "source": [
    "remainder='passthrough' é uma maneira eficaz de garantir que todas as colunas necessárias sejam mantidas no DataFrame final após a aplicação de transformações específicas, facilitando o gerenciamento de pipelines de dados complexos onde apenas algumas colunas precisam de transformação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2de25e-81d8-4583-9951-cac360531485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_features = [\"Make\",\"Colour\", \"Doors\"]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\",\n",
    "                                  one_hot,\n",
    "                                  categorical_features)],\n",
    "                                  remainder = \"passthrough\")\n",
    "transformed_X = transformer.fit_transform(car_sales_missing)\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d38975b-f24f-4c4c-bfbe-12a7805e60af",
   "metadata": {},
   "source": [
    "### Option2 : Fill values with scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb8c63-40b9-4c96-87b6-9a9ad8e93239",
   "metadata": {},
   "source": [
    "Normalization (also called min-max scaling) - This rescales all the numerical values to between 0 and 1, with the lowest value being close to 0 and the highest previous value being close to 1. Scikit-Learn provides functionality for this in the MinMaxScalar class.\r\n",
    "\r\n",
    "Standardization - This subtracts the mean value from all of the features (so the resulting features have 0 mean). It then scales the features to unit variance (by dividing the feature by the standard deviation). Scikit-Learn provides functionality for this in the StandardScalar clas\n",
    "\n",
    "O escalonamento de características geralmente não é necessário para a sua variável alvo.\r\n",
    "\r\n",
    "O escalonamento de características geralmente não é necessário com modelos baseados em árvores (por exemplo, Floresta Aleatória), pois eles podem lidar com características variáveis.s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1c409-df53-4b89-aa4d-25f91357d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing= pd.read_csv('zero-to-mastery-ml-master/data/car-sales-extended-missing-data.csv')\n",
    "car_sales_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d756c767-691d-4c5a-96d5-b32938bb5e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74a8529-a8ad-43b0-afd7-75ae042f8ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the rows with no labels\n",
    "car_sales_missing.dropna(subset = [\"Price\"], inplace = True)\n",
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8fd2d9-d036-4fd1-8ab9-43e562e67efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into X and y\n",
    "X = car_sales_missing.drop(\"Price\" , axis =1 )\n",
    "y = car_sales_missing[\"Price\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6ed9fa-c134-48c4-ab32-d717bf2786d4",
   "metadata": {},
   "source": [
    "Sobre o codigo abaixo\n",
    "*SimpleImputer* : Usado para preencher valores ausentes em colunas usando diferentes estratégias;\n",
    "*ColumnTransformer* : Permite aplicar diferentes transformações a colunas específicas de um dataframe.\n",
    "\n",
    "-> A seguir, a explicação serve para as outra linhas com comandos semelhantes\n",
    "*cat_imputer* : Preenche valores ausentes em variveis categoricas com a palabra 'missing'\n",
    "*cat_features* : Linha destinada para as quais o 'cat_imputer' será aplicado.\n",
    "\n",
    "*ColumnTransform* : Compila os imputadores definidos anteriormente.\n",
    "O ColumnTransformer é aplicado ao dataframe X (que não é mostrado no código mas é assumido como o conjunto de dados de entrada). Ele ajusta os imputadores ao dataframe (fit) e transforma os dados (transform), resultando em um novo conjunto de dados (filled_X) onde todos os valores ausentes foram preenchidos conforme especificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ea1809-f32a-4e23-83e1-da78466eafdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with scikit learn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#Fill categorical values 'missing' and numerical values with mean\n",
    "cat_imputer = SimpleImputer(strategy = \"constant\", fill_value = \"missing\")\n",
    "door_imputer = SimpleImputer(strategy = 'constant', fill_value =4 )\n",
    "num_imputer = SimpleImputer(strategy = 'mean')\n",
    "\n",
    "#Define columns\n",
    "cat_features = [\"Make\",\"Colour\"]\n",
    "door_features = [\"Doors\"]\n",
    "num_features = [\"Odometer (KM)\"]\n",
    "\n",
    "#Create an imputer(something that fills missing data)\n",
    "imputer = ColumnTransformer([\n",
    "    (\"cat_imputer\", cat_imputer, cat_features),\n",
    "    (\"door_imputer\", door_imputer, door_features),\n",
    "    (\"num_imputer\",num_imputer,num_features)\n",
    "])\n",
    "\n",
    "#Transform the data\n",
    "filled_X = imputer.fit_transform(X)\n",
    "filled_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0739a61b-bb4d-42c4-932b-2996c0fd6330",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_filled = pd.DataFrame(filled_X,\n",
    "                                columns = [\"Make\",\"Colour\",\"Doors\",\"Odometer (KM)\"])\n",
    "car_sales_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94faf226-6c78-4b8f-b090-524e2a7f6f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_filled.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ddaa0d-578a-457e-bc37-665d91246501",
   "metadata": {},
   "outputs": [],
   "source": [
    "X - = car_sales_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897f4ba8-7f73-4222-bd90-c0ae7abc4fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precisamos passar as varaiveis string para numeros\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Supondo que suas colunas categóricas sejam \"Make\", \"Colour\", e \"Doors\"\n",
    "categorical_features = [\"Make\", \"Colour\", \"Doors\"]\n",
    "\n",
    "# Criar o transformador para as colunas categóricas\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"encoder\", OneHotEncoder(), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # Mantém as outras colunas que não são especificadas\n",
    ")\n",
    "\n",
    "# Suponha que X é o seu DataFrame de características\n",
    "# Aplicar a transformação\n",
    "transformed_X = column_transformer.fit_transform(car_sales_filled)\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6731e15c-a0a4-480f-a7a2-0bdf82fac999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we've got our data as numbers and filled (no missing values)\n",
    "#let's fit a model\n",
    "np.random.seed(42)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(transformed_X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.2)\n",
    "model = RandomForestRegressor(n_estimators = 100)\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)\n",
    "                                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019baf0a-e5f2-4794-a2a0-e52dd39a7a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(car_sales_filled), len(car_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f45c60-d67d-4eb4-9358-f8b1fa344026",
   "metadata": {},
   "source": [
    "## 2. Hoosing the right estimator/algorithm for your problem\n",
    "['0. An end-to-end Scikit-Learn workflow',\n",
    " '1. Getting the data ready',\n",
    " '2. Choose the right estimator/algorithm for our problems',\n",
    " '3. Fit the model/algorithm and use it to make prediction on our data',\n",
    " '4. Evaluating a model',\n",
    " '5. Improve a model',\n",
    " '6. Save and load a trained model',\n",
    " '7. Putting it all together!\"]\n",
    "\n",
    " * Some things to note:\n",
    " * Sklearn refers to machine learning models, algorithms as estimators.\n",
    " * Classification problem - predicting a category (heart disease or not)\n",
    " * Sometimes you 'll see 'clf' (short for classifier) used as classification estimator\n",
    " * Regression problem - prediciting a number (selling price of a car)\n",
    "\n",
    "If you're working on a machiene learning problem and looking skelearn and not usre what model you should use, refer to the this site\n",
    "https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e65e6fd-1a09-4d32-accd-49ef0ce660da",
   "metadata": {},
   "source": [
    "### 2.1 Picking a machine learning model for a regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c45209c-2342-4202-bd8e-b3354837edfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get California housing dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1fc34d-95d5-478f-b014-06b6b01d1856",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "housing_df = pd.DataFrame(housing[\"data\"], columns = housing[\"feature_names\"])\n",
    "housing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac981a-e5a3-414a-99ac-5605f4ad5888",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df[\"target\"] = housing[\"target\"]\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e4e316-8416-4089-af91-9f6c78090026",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = housing_df.drop(\"MedHouseVal\" , axis = 1)\n",
    "housing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23800801-bfc0-42c2-bed0-6a16d3c57289",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157bfa5c-f594-4c11-931c-04bfc742a1e5",
   "metadata": {},
   "source": [
    "O método Ridge, também conhecido como regressão ridge ou regularização Tikhonov, é uma técnica usada em estatística e aprendizado de máquina para analisar dados que sofrem de multicolinearidade — quando há alta correlação entre as variáveis independentes. Este método é uma extensão da regressão linear e é utilizado para prevenir o problema de sobreajuste, que pode ocorrer em modelos de previsão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1c79d5-49b2-4fe1-85f2-2c78de88cbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import algorithm\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "#Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "#Create the data\n",
    "X = housing_df.drop(\"target\", axis = 1)\n",
    "y = housing_df[\"target\"] #median house price in $100,00s\n",
    "\n",
    "#Split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "#Instatiate and fit the model\n",
    "model = Ridge()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Check the score of the model(on the test set)\n",
    "model.score(X_test, y_test)\n",
    "\n",
    "#instatiante and fit the model (on t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0919434b-6953-4bf1-afb9-aeaa30861248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the randomforestregressor model class from the ensamble module\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "#create the data\n",
    "X = housing_df.drop(\"target\", axis =1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "#Split into traint and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2)\n",
    "\n",
    "#Create a random forest model\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Check the score\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66273857-5948-4a26-b558-0006bfd448ff",
   "metadata": {},
   "source": [
    "## 2.2 Picking a machine learning model for a classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69abda89-d374-4aaa-87a9-f3535f877fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease = pd.read_csv('zero-to-mastery-ml-master/data/heart-disease.csv')\n",
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c86262e-27a1-436b-8c28-0e75720990e7",
   "metadata": {},
   "outputs": [],
   "source": [
    " #import the linearSVC estimator class\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "#Make the data\n",
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "#Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "#Instatiante LinearSVC\n",
    "clf = RandomForestClassifier(n_estimators = 1000)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "#Evoluate the Linear\n",
    "clf.score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1db3435-d250-44d2-8e74-e01a2918f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease[\"target\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8643bfea-d0a8-4a5f-b922-e38f0ec0d522",
   "metadata": {},
   "source": [
    "## 3. Fit the model/algorithm on our data and use it to make predict  \n",
    " ### Fiting a model data\n",
    "\n",
    " Different names for:\n",
    " *`X` = Features, features variables, data\n",
    " *`y` = labels, targets, target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8c511-3f1f-4d01-b77d-83e8a727d394",
   "metadata": {},
   "outputs": [],
   "source": [
    " #import the linearSVC estimator class\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "#Make the data\n",
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "#Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "#Instatiante LinearSVC\n",
    "clf = RandomForestClassifier(n_estimators = 100)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "#Evoluate the Linear\n",
    "clf.score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2432d36f-3a66-4524-b4fb-0da427d8ca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890126ed-c917-4c52-8451-e6f2dbd5f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca59e6d4-4544-448a-bbd9-173a7520598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebbe781-7f7b-4cd0-bfc7-2a07f4b420bd",
   "metadata": {},
   "source": [
    "### 3.2 Make predictions using a machine learning model\n",
    "2 ways to make predictions:\n",
    "1. `predict()´\n",
    "2. `predict_proba()´\n",
    "\n",
    "predict: Quando você precisa de uma decisão direta/etiqueta/classificação.\r\n",
    "predict_proba: Quando você precisa avaliar a incerteza do modelo ou tomar decisões que dependem do nível de confiança nas previsões (por exemplo, em situações onde custos diferentes estão associados a diferentes tipos de erros de classificação)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2b14cf-20ed-40dc-9bea-90dd6a5245f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use a trained model to make predictions\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53be025-cc21-421b-8fdc-5de9b5f96b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed947d1-522e-4047-9608-173a1ae16c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9d2dac-f48f-4ee5-a763-da1ef5c1413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions to truth labels to evaluate the model\n",
    "y_preds = clf.predict(X_test)\n",
    "np.mean(y_preds == y_test)# Estamos calculando aqui a precisao ou quao bem o modelo saiu com esse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0230810a-4933-44e4-9d26-d537cf854e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a718d6fc-a155-4419-a216-15c11e54b0e1",
   "metadata": {},
   "source": [
    " Make prediction with `predict_proba()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38abbf34-5118-4e71-864d-9b5df1d2d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_proba() returns probabilities of a classification label\n",
    "clf.predict_proba(X_test[:5]) #Temos como respostas estimativas de probablidade\n",
    "#No caso a coluna da esquerda representa a porcentagem da saida ser 0\n",
    "# E a coluna da direita da saida ser 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d3bbfb-e853-4c2e-afce-c6695d6ad345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's predict() on the same data...\n",
    "clf.predict(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb9e856-b823-4fef-b4d4-0ad6c30369a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6baa5c-4f8d-4af0-87a8-6178930d3ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98369be3-c80e-47df-a73f-565939361cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "np.random.seed(42)\n",
    "\n",
    "#Create de data\n",
    "X = housing_df.drop(\"target\", axis = 1)#Pegando todas as colunas exceto a coluna target\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "#Split into training and test sets\n",
    "X_train,X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "#Create miodel instance\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "#Fit the model to the data\n",
    "model.fit(X_train, y_train)\n",
    "#Make predictions\n",
    "y_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b716c-e680-4794-94b0-13bf5bc190b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b16643-ab02-494b-8bb9-b74bc6789382",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45548ded-98a5-4db2-a31e-caad6b306b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare the prediction to the truth\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test,y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7499a712-a9ea-4546-bfc8-c4bc681b2920",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9661b46e-e013-42a5-9386-0f6c48e2bf93",
   "metadata": {},
   "source": [
    "## 4.Evoluating a machine learning model\n",
    "\n",
    "Three ways to evoluate Scikit-Learn models/estimator\n",
    "1. Estimator's built-in `score` method\n",
    "2. The `scoring` parameter\n",
    "3. Problem-specific metric functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7261e51f-d8ee-4675-96b0-f912021653c5",
   "metadata": {},
   "source": [
    "### 4.1 Evoluating a model with the score method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe140e-9874-4013-ae4b-3f5ec84a8504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.random.seed(42)\n",
    "\n",
    "#Create de data\n",
    "X = heart_disease.drop(\"target\", axis = 1)#Pegando todas as colunas exceto a coluna target\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "#Split into training and test sets\n",
    "X_train,X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "#Create miodel instance\n",
    "model = RandomForestClassifier(n_estimators =100)\n",
    "\n",
    "#Fit the model to the data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb51c0f-72c6-4cbd-ac6e-02e9215f7b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The highest value for the .score() method is 1.0, the lowest 0.0\n",
    "#The default score() evaluating metric is r_squared for regression algorithms\n",
    "model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9175696-a144-4fd1-bd8f-715882eaec84",
   "metadata": {},
   "source": [
    "## 4.2 Evaluating a model using the scoring parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d675b3f2-f232-4260-934d-aa370be4edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.random.seed(42)\n",
    "\n",
    "#Create de data\n",
    "X = heart_disease.drop(\"target\", axis = 1)#Pegando todas as colunas exceto a coluna target\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "#Split into training and test sets\n",
    "X_train,X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "#Create miodel instance\n",
    "model = RandomForestClassifier(n_estimators =100)\n",
    "\n",
    "#Fit the model to the data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c04f90-02d9-4520-b03d-89ea0f32a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(model,X,y, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acde2faf-9734-4732-9544-d022b0e37658",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "#Single training and teste split score\n",
    "clf_single_score = model.score(X_test, y_test)\n",
    "\n",
    "#Take de mean of 5-fold cross-validation score\n",
    "clf_cross_val_score = np.mean(cross_val_score(clf,X, y, cv = 5))\n",
    "\n",
    "#Compare the two\n",
    "clf_single_score, clf_cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d620e4-9e1e-4867-830e-4d9eac1c824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scoring parameter set to none by default\n",
    "cross_val_score(model,X,y, cv = 5,scoring = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabba064-9cce-43b3-99fc-3d40adc19816",
   "metadata": {},
   "source": [
    "### 4.2.1 Classification model evoluating metrics\n",
    "1.Accuracy\n",
    "2.Area under ROC curve\n",
    "3.Confusion matrix\n",
    "4.Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844133a3-bf09-4669-bbd0-9626e4312700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "cross_val_score = cross_val_score(model,X,y, cv =5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad01ac50-e808-4eb3-a8cf-018f6385ec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score)\n",
    "print(f\"Heart Disease Classifier Cross-Validated Accuracy: {np.mean(cross_val_score) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e11d66a-0138-4a2e-99be-8fc4eb3720fc",
   "metadata": {},
   "source": [
    "**Area under the receiver, operating characteristic curve (AUC/ROC)**\n",
    "\n",
    "* Area under curve(AUC)\n",
    "* ROC curve\n",
    "ROC curves are a comparsion of a model's true positive rate (tpr) veruss a models false positive rate(fpr.\n",
    "\n",
    "*True positive = model predicts 1 when truth is 1\n",
    "*False positive = model predictis 1 when truth is 0\n",
    "*True negative = model predicts 0 when truth is 0\n",
    "*False negative = model predicts 0 when truth is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6464d6b-0bca-4da7-8bb5-7e5f77fda965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X_test... etc\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f24154-3e9b-4c9a-8520-5abc4eb90298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "#Fit the classifier\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "#Make predictions with probabilites\n",
    "y_probs = clf.predict_proba(X_test)\n",
    "y_probs[:10], len(y_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefe7694-3bc7-4530-a0e2-9f25a51a4388",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs_positive = y_probs[:,1]\n",
    "y_probs_positive[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d6cd7d-2ba5-4a09-b84e-c0a0231fc5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fpr,tpr and thresholds\n",
    "#A função roc_curve calcula a taxa de falsos positivos (FPR), a taxa de verdadeiros positivos (TPR) e os limiares de decisão para diferentes pontos de corte nas probabilidades preditas.\n",
    "#Muito usada em classificação binária\n",
    "fpr,tpr, thresholds = roc_curve(y_test,y_probs_positive)\n",
    "\n",
    "#Check de false positive rates\n",
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b8b8ff-a2a3-40a1-b5b9-3380a8532d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crate a function for plotting ROC curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_curve(fpr, tpr):\n",
    "    \"\"\"\n",
    "    Plots a ROC curve given the false positive rate (fpr)\n",
    "    and true positive rate (tpr) of a model.\n",
    "    \"\"\"\n",
    "    #Plot roc curve\n",
    "    plt.plot(fpr,tpr, color = \"orange\", label = \"ROC\")\n",
    "    #Plot line with no predictive power (baseline)\n",
    "    plt.plot([0,1],[0,1], color = \"darkblue\", linestyle = \"--\", label = \"Guessing\")#: Define o rótulo da linha como \"Guessing\". Esse rótulo será usado na legenda do gráfico.\n",
    "    #representa os valores do eixo y. Isso significa que a linha vai do ponto (0,0) até o ponto (1,1).\n",
    "#Customize the plot\n",
    "plt.xlabel(\"False positive rate ( fpr)\")\n",
    "plt.ylabel(\"True positive rate (tpr)\")\n",
    "plt.title(\"Receiber Operating Characteristic (ROC) Curve\")\n",
    "plt.legend()\n",
    "plt.plot()\n",
    "\n",
    "plot_roc_curve(fpr, tpr)\n",
    "#Serve como uma linha de base para comparar o desempenho do modelo. Um modelo com desempenho acima desta linha tem algum poder preditivo, enquanto um modelo abaixo desta linha tem desempenho pior que o aleatório."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4725fe21-04dc-457c-bf17-c735121e7be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score##Area sobre a curva, no caso tudo abaixo da curva amarelka\n",
    "\n",
    "roc_auc_score(y_test, y_probs_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b187a623-92fd-4ab9-9d96-0c635bb5a628",
   "metadata": {},
   "source": [
    "ROC curves and AUC metrics are evaluation metrics for binary classification models (a model which predicts one thing or another, such as heart disease or not).\n",
    "\n",
    "The ROC curve compares the true positive rate (tpr) versus the false positive rate (fpr) at different classification thresholds.\n",
    "\n",
    "The AUC metric tells you how well your model is at choosing between classes (for example, how well it is at deciding whether someone has heart disease or not). A perfect model will get an AUC score of 1.\n",
    "                                                                             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b546e7c5-2965-429c-8553-a63a58f3bd6d",
   "metadata": {},
   "source": [
    "#Plot perfect Roc curve and auc score\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test)\n",
    "plot_roc_curve(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb37d292-4d5e-49f0-ba98-82e36da490d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perfect AUC score\n",
    "roc_auc_score(y_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44081a63-5462-400f-9bb7-f09ef7c3b333",
   "metadata": {},
   "source": [
    "**Confusion matrix**\n",
    "\n",
    " A confusion matrix is a quick way to compare the labels a model predicts and the actuaç labels it was supposed to predict.\n",
    " In essence, giving you an idea of where the model is getting confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2fdca3-9fe6-4f69-a438-d2db625e6227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec5d3ff-7263-4bfa-9d68-c7463dca8801",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_test,\n",
    "            y_preds,\n",
    "            rownames=[\"Actual Label\"],\n",
    "            colnames=[\"Predicted Labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3eb26a-c93e-4448-b795-8b7e398d47fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opcao 1\n",
    "#Make our confusion matrix more visual with Seaborn's heatmap()\n",
    "import seaborn as sns\n",
    "#Set the font scale\n",
    "sns.set(font_scale = 1.5)\n",
    "\n",
    "#Create a confusion matriddx\n",
    "conf_mat = confusion_matrix(y_test, y_preds)\n",
    "\n",
    "#Plot it using Seaborn\n",
    "sns.heatmap(conf_mat);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec70d6d4-2325-4bcc-a487-2244e0539289",
   "metadata": {},
   "source": [
    "#Opcao 2\n",
    "##Creating a confusion matrix using Scikit-Learn\n",
    "To use the new methods of creating a confusion matrix with Scikit-Learn you will need sklearn version 1.0+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb5426f-d101-44d4-9650-f2d890b9362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7b6891-c934-4010-8fa2-e5a7bf3cbcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_estimator(estimator = model,X=X,y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4fb5ff-019c-4ed5-bce1-5e697d5cf838",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_true = y_test,\n",
    "                                        y_pred = y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c339ab-c059-439e-ad80-19050eab9055",
   "metadata": {},
   "source": [
    "**Classification Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d483b9-5185-4dff-aa59-7dbf2d0a7ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fed4f8-7c14-49ea-931d-b181e87c2744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where precision and recall become valuable\n",
    "import numpy as np\n",
    "disease_true = np.zeros(10000)\n",
    "disease_true[0] = 1 # only one positive\n",
    "\n",
    "disease_preds = np.zeros(10000)#Model predicts every case as 0\n",
    "pd.DataFrame(classification_report(disease_true,\n",
    "                                  disease_preds,\n",
    "                                  output_dict = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3248c-5689-4708-96b8-62d74dead6a8",
   "metadata": {},
   "source": [
    "To summarize classification metrics:\n",
    "\n",
    "* Accuracy is a good measure to start with if all classes are abalanced(e.g. same amount of samples which are labelled with 0 or 1).\n",
    "* Precision and recall become more important when calsses are imbalanced\n",
    "* If false positive predictions are worse than false negatives, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea28a39e-e56a-4b46-8625-305e8a847b4a",
   "metadata": {},
   "source": [
    "## Classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d8fb0c-0d49-4580-9f05-24dc00eba4f4",
   "metadata": {},
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a131cd3-9111-4c56-b63d-3db5ab39680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#where precision and recall become valuable\n",
    "disease_true = np.zeros(10000)\n",
    "disease_true[0] = 1 # only one positive case\n",
    "\n",
    "disease_preds = np.zeros(10000)#Model predicts every case as 0\n",
    "\n",
    "pd.DataFrame(classification_report(disease_true,\n",
    "                                  disease_preds,\n",
    "                                  output_dict = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0f430e-81ed-4568-9589-0ebd2da12096",
   "metadata": {},
   "source": [
    "## 4.2.2 Regression model evoluation\n",
    "\n",
    "The ones we re going to cover are:\n",
    "1. r² (pronounced r-squered) or coefficient of determination\n",
    "2. Mean absolute errro (MAE)\n",
    "3. Mean sqaured error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c4d9a-e794-430d-b1f1-87056f0be9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "housing_df = pd.DataFrame(housing[\"data\"], columns = housing[\"feature_names\"])\n",
    "housing_df[\"target\"] = housing[\"target\"]\n",
    "np.random.seed(42)\n",
    "\n",
    "X = housing_df.drop(\"target\", axis = 1)\n",
    "y= housing_df[\"target\"]\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators = 100)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a97bd0-54c5-41d4-8955-3d98228a58e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699d8b9f-c2ed-4cdb-b78c-9be3d062cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46076d2c-9d50-4341-806c-2f9e35b3d347",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f71a2f0-fa2d-4d09-92f0-20740ee8d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7e21cf-6f64-40ab-9ebe-08e359935065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#Fill an array with y_test mean\n",
    "y_test_mean = np.full(len(y_test), y_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9be03d-e2c9-498c-a6af-1845a5a3166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_mean[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b40a758-83aa-47f1-9186-44ec12399dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_true = y_test,\n",
    "         y_pred= y_test_mean)\n",
    "#Um R² de 0 significa que o modelo não faz melhor do que simplesmente prever a média para todas as observações.\n",
    "#Neste caso específico, o cálculo do R² vai resultar em 0, pois o modelo de previsão (prever a média) não explica nenhuma \n",
    "#variação dos valores reais em torno da própria média. Este é um teste básico para verificar a linha de base de desempenho para \n",
    "#modelos mais complexos comparados com o simples ato de prever a média.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb7fff8-09e1-4258-9b0b-8ff1314db810",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_true = y_test,\n",
    "         y_pred = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f33471-eb40-4474-a3a4-566571f953b1",
   "metadata": {},
   "source": [
    "**Mean abosolute error (MAE)**\n",
    "\n",
    "MAE is the average of the absolute differences between predictions and actual values.\n",
    "It given you an idea of how wrong your models predictions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e444cfd-c222-4aaa-b641-65220493d5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_preds = model.predict(X_test)\n",
    "mae= mean_absolute_error(y_test, y_preds)\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b197b1d-7ff1-4125-a2dd-d5eff0ef38b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\"actual values\" : y_test,\n",
    "                        \"predicted values\":y_preds})\n",
    "df[\"differences\"] = df[\"predicted values\"] - df[\"actual values\"]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2b8263-3721-4b8f-ac48-68a4acb6539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4afee12-5c44-4375-b316-9014d2d756c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06710f3-67ef-455d-abeb-1e4eb83a7200",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"differences\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b2ea06-df92-4fb8-b9e7-d12d82d058e3",
   "metadata": {},
   "source": [
    "**Mean squared error (MSE)**\n",
    "\n",
    "MSE is the mean of the square of the errors between actual and predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d6d056-2a61-4132-9517-cd88c4fd3835",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean squared error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_preds = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_preds)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719187d6-4d2d-4fe2-89cd-b23e5065873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"squared_differences\"] = np.square(df[\"differences\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13119933-123a-4356-aee8-896a782112b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE by hand\n",
    "squared = np.square(df[\"differences\"])\n",
    "squared.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c473abe2-d748-46c6-9e1a-feb499cfbeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large_error = df.copy()\n",
    "df_large_error.iloc[0][\"squared_differences\"] = 16\n",
    "df_large_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a245c348-a6a3-4e79-8c87-758d2adbdbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate MSE with large error\n",
    "df_large_error[\"squared_differences\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1085e838-fcad-4fb1-b43e-6bc4fcf39458",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large_error.iloc[1:100] = 20\n",
    "df_large_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275ba38e-1771-413e-825b-136b261a8d73",
   "metadata": {},
   "source": [
    "### 4.2.3 Finally using the `scoring` parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d46de1-a5e4-49d6-aeaa-4d51c25e273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = heart_disease.drop(\"target\" , axis = 1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4600b782-0a19-4929-8328-77e65b19c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "#Cross validation accuracy\n",
    "cv_acc= cross_val_score(clf, X, y, cv = 5, scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cff5e10-28ef-4c2f-8dec-53cc00541a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-validated accuracy\n",
    "print(f'The cross- validated accuracy is : {np.mean(cv_acc) * 100:2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73486675-31bc-463a-859b-e3b1462e1dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision\n",
    "cv_precision = cross_val_score(clf, X, y, cv = 5, scoring = \"precision\")\n",
    "cv_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132e42d2-53b8-4322-821b-416ebcc54a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-validated precision\n",
    "print(f'The cross-validated precision is: {np.mean(cv_precision)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a170e99-bd96-4a4e-851c-22a371d78bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall \n",
    "cv_recall = cross_val_score(clf, X, y, cv = 5, scoring =\"recall\")\n",
    "cv_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2488e14-77fb-49be-a4f2-be65b41f6e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-validated precision\n",
    "print(f\"The cross-validated precision is : {np.mean(cv_precision)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2db7cf1-641e-4d36-b8c6-4074fc264ba3",
   "metadata": {},
   "source": [
    "Lets see the scoring parameter using for a regression problem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cdaeac-9de3-466d-abaa-ca4a4742a6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "housing_df = pd.DataFrame(housing[\"data\"], columns = housing[\"feature_names\"])\n",
    "housing_df[\"target\"] = housing[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a049766-a768-4f31-b47d-edbc790c4ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = housing_df.drop(\"target\", axis =1)\n",
    "y = housing_df[\"target\"]\n",
    "model = RandomForestRegressor(n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fb305a-4a81-48bc-b983-83437af418a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cv_r2 = cross_val_score (model,X,y,cv = 3, scoring = None)\n",
    "np.mean(cv_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435cb4a2-c2a7-4d96-aaf7-975d851f9bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c32b78-3373-4bd2-aa9f-d2587e274718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean squared error\n",
    "cv_mse = cross_val_score(model,X,y,cv=3, scoring  =\"neg_mean_squared_error\")\n",
    "np.mean(cv_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f511109-5499-4d20-9f91-44613f4ca640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean squared error\n",
    "cv_mae = cross_val_score(model,X,y,cv=3, scoring  =\"neg_mean_absolute_error\")\n",
    "np.mean(cv_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9ef230-3ae2-4076-aaf0-8b68f543a835",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035f1de4-d7e5-475e-9141-c3932002292f",
   "metadata": {},
   "source": [
    "## 4.3 Using a differente evaluatrion metrics as Scikit-Learn functions\n",
    "The 3rd way to evoluate scikit-learn machine learnfing models/estimators is using the sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed42a6f5-837d-4164-b7db-21540385512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "#Create X, y:\n",
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "y= heart_disease[\"target\"]\n",
    "#Split data\n",
    "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size = 0.2)\n",
    "#Create model\n",
    "clf = RandomForestClassifier()\n",
    "#Fit model\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "#Make predicitons\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "#Evaluate model using evaluation functions\n",
    "\n",
    "print(\"Classifier mettrics on the test set\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, clf.predict(X_test)) * 100:.2f}%\")\n",
    "print(f\"Precision: {precision_score(y_test,y_preds)}\")\n",
    "print(f\"Recall: {recall_score(y_test,y_preds)}\")\n",
    "print(f\"F1_score: {f1_score(y_test,y_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ac6117-964b-44e8-8a59-95f1da6410bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "#Create X, y:\n",
    "X = heart_disease.drop(\"target\", axis = 1)\n",
    "y= heart_disease[\"target\"]\n",
    "#Split data\n",
    "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size = 0.2)\n",
    "#Create model\n",
    "clf = RandomForestClassifier()\n",
    "#Fit model\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "#Make predicitons\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "print('Regression mertrics on the teste set')\n",
    "print(f\"R2 score: {r2_score(y_test, y_preds)}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test,y_preds)}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7de5c96-6a85-4fe6-a182-6290fabfaab5",
   "metadata": {},
   "source": [
    "## 5.0 Improve a model\n",
    "First predictions = baseline predictions.\n",
    "First model = baseline model.\n",
    "\n",
    "From data perspective:\n",
    "* Could we collect more data ? ( generally, the more data, the better)\n",
    "* Could we improve our data\n",
    "\n",
    "From a model perspective:\n",
    "* Is there a better model we could to use\n",
    "* Could we improve the current model?\n",
    "\n",
    "    Parameters x Hyperparameters\n",
    "* Parameters = model find these patterns in data\n",
    "* Hyperparameters = settings on a model you can adjust to (potentially) improve its ability to find patterns\n",
    "\n",
    "Three ways to adjust hyperparameters\n",
    "1. By hand\n",
    "2. Randomyly with RandomSearchCv\n",
    "3. Exhaustively with GridSearchCv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563243ef-a53b-44f6-acf8-312342db8677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850158e3-1530-478d-ac24-4eb61805ea03",
   "metadata": {},
   "source": [
    "### 5.1 Tuning hyperparameters by hand\n",
    "\n",
    "Let's make 3 sets,training validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aac694-fc42-4ad0-91b4-4f4c87fd4999",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30bb18c-678e-4e9d-a5f4-2e487fee69be",
   "metadata": {},
   "source": [
    "We're coing to try and adjust:\n",
    "\n",
    "* max_depth\n",
    "* max_features\n",
    "* min_simples_leaf\n",
    "* min_samples_split\n",
    "* n_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429b4e75-f1b0-4af3-8859-a32041e74e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_preds(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    Performs evaluation comparison on y_true labels vs. y_pred labels.\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_preds)\n",
    "    precision = precision_score(y_true, y_preds)\n",
    "    recall = recall_score(y_true, y_preds)\n",
    "    f1 = f1_score(y_true, y_preds)\n",
    "    metric_dict = {\n",
    "        \"accuracy\": round(accuracy, 2),\n",
    "        \"precision\": round(precision, 2),\n",
    "        \"recall\": round(recall, 2),\n",
    "        \"f1\": round(f1, 2)\n",
    "    }\n",
    "    print(f\"Acc: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precision: {precision * 100:.2f}%\")\n",
    "    print(f\"Recall: {recall * 100:.2f}%\")\n",
    "    print(f\"F1 Score: {f1 * 100:.2f}%\")\n",
    "    return metric_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61607af0-7041-4d25-a1df-c7ec916a37da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for random number generator\n",
    "np.random.seed(42)\n",
    "\n",
    "# Shuffle the data\n",
    "heart_disease_shuffled = heart_disease.sample(frac=1)\n",
    "\n",
    "# Split into X and y\n",
    "X = heart_disease_shuffled.drop(\"target\", axis=1)\n",
    "y = heart_disease_shuffled[\"target\"]  # Use the shuffled DataFrame to ensure the target matches the features\n",
    "\n",
    "# Calculate indices for train, validation, and test split\n",
    "train_split = round(0.7 * len(heart_disease_shuffled))  # 70% of data\n",
    "valid_split = round(train_split + 0.15 * len(heart_disease_shuffled))  # 15% of data\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_valid, y_valid = X[train_split:valid_split], y[train_split:valid_split]\n",
    "X_test, y_test = X[valid_split:], y[valid_split:]\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train,y_train)\n",
    "y_preds = clf.predict(X_valid)\n",
    "\n",
    "#Evoluate the classifier on validadtion set\n",
    "baseline_metrics = evaluate_preds(y_valid, y_preds)\n",
    "baseline_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221637c7-7905-4d4b-a963-c8675a1cef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9fcdb7-3359-4a86-a8ea-2f6db2b37d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed\n",
    "\n",
    "#Create a second classifier with different hyperparameters\n",
    "clf_2 = RandomForestClassifier()\n",
    "clf_2.fit(X_train,y_train)\n",
    "y_preds2 = clf_2.predict(X_valid)\n",
    "clf_2_metrics = evaluate_preds(y_valid,y_preds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312cef07-3d89-40c8-9b52-81bd48c32bf5",
   "metadata": {},
   "source": [
    "### 5.2 Hyperparameter tuning with randomizedSearchCv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da4dc90-2700-42de-91fa-5b3f46b1b99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "grid = {\"n_estimators\" : [10,100,200,500,1000,1200],\n",
    "        \"max_depth\": [None,5,10,15,20,30],\n",
    "        \"max_features\":[\"auto\",\"sqrt\"],\n",
    "        \"min_samples_split\": [2,4,6],\n",
    "        \"min_samples_leaf\":[1,2,4]\n",
    "       }\n",
    "np.random.seed(42)\n",
    "\n",
    "#Split into X e y \n",
    "X = heart_disease_shuffled.drop(\"target\", axis=1)\n",
    "y = heart_disease_shuffled[\"target\"]  \n",
    "\n",
    "#Split data\n",
    "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size = 0.2)\n",
    "#Create model\n",
    "clf = RandomForestClassifier(n_jobs= None)\n",
    "\n",
    "#Setup randomized \n",
    "rs_clf = RandomizedSearchCV(estimator = clf,\n",
    "                            param_distributions=grid,\n",
    "                            n_iter=10, #number of models to try\n",
    "                            cv=5,\n",
    "                            verbose =2)\n",
    "#Fit the randomizeSearch\n",
    "rs_clf.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2646966e-51f1-4d0b-8db8-4756628d2897",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5439628-eb8c-45d9-8c0e-c1c2265689c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predicition with the best hyperparameter\n",
    "rs_y_preds = rs_clf.predict(X_test)\n",
    "\n",
    "#Evaluate the prediction\n",
    "rs_metrics = evaluate_preds(y_test, rs_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9507be-7729-456d-86f9-0a57a372cecc",
   "metadata": {},
   "source": [
    "### 5,3 hyperparmater tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa7975-0e2f-4d62-8a1e-30d8d90dff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ea9c1-c27e-4f88-be22-88c9d3219c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_2 = {'n_estimators': [100, 200, 500],\n",
    " 'max_depth': [None],\n",
    " 'max_features': ['auto', 'sqrt'],\n",
    " 'min_samples_split': [6],\n",
    " 'min_samples_leaf': [1, 2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b97cf7-8cc8-47b9-b4a4-0324eaf80a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "#Split into X e y \n",
    "X = heart_disease_shuffled.drop(\"target\", axis=1)\n",
    "y = heart_disease_shuffled[\"target\"]  \n",
    "\n",
    "#Split data\n",
    "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size = 0.2)\n",
    "#Create model\n",
    "clf = RandomForestClassifier(n_jobs= None)\n",
    "\n",
    "#Setup randomized \n",
    "gs_clf = GridSearchCV(estimator = clf,\n",
    "                     param_grid=grid_2,\n",
    "                     cv=5,\n",
    "                     verbose =2)\n",
    "#Fit the randomizeSearch\n",
    "gs_clf.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51ed654-78d3-424f-9c11-6eb236656a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e844b2b-66d8-4af0-9d30-182ab10878e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_y_preds = gs_clf.predict(X_test)\n",
    "\n",
    "#Evaluate the prediction\n",
    "gs_metrics = evaluate_preds(y_test,gs_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e903b04-28fd-4dac-96dd-efef8c7649b7",
   "metadata": {},
   "source": [
    "### comparando diferentes models metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10fbf7-8338-430c-95f3-af8a55fab12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_metrics = pd.DataFrame({\"baseline\":baseline_metrics,\n",
    "                                \"clf_2\": clf_2_metrics,\n",
    "                                \"random search\": rs_metrics,\n",
    "                                \"grid search\": gs_metrics})\n",
    "compare_metrics.plot.bar(figsize = (10,8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368ea20a-5065-4053-83ae-c9f3c8fe820b",
   "metadata": {},
   "source": [
    "## 6. Saving and loading train machine learnign models\n",
    "Two ways to save and load machine learning models\n",
    "1. With Python pickle module\n",
    "2. With the joblib module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab3bdf3-aafb-4e25-9567-9dfd450c8161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#save the model\n",
    "pickle.dump(gs_clf, open(\"gs_random_forest_modek_1.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9e9ce6-91d2-4e8f-83b8-8d423f1d6225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load a saved model\n",
    "loaded_pickle_model = pickle.load(open(\"gs_random_forest_modek_1.pkl\",\"rb\"))\n",
    "  \n",
    "pickle_y_preds = loaded_pickle_model.predict(X_test)\n",
    "evaluate_preds(y_test, pickle_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d039c0-fd52-455c-8745-c096f747a5f1",
   "metadata": {},
   "source": [
    "## 7.Putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d22bc053-6c93-4a68-a4b4-9884cf11f19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Make</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Odometer (KM)</th>\n",
       "      <th>Doors</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>35431.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15323.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BMW</td>\n",
       "      <td>Blue</td>\n",
       "      <td>192714.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19943.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>84714.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>28343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>White</td>\n",
       "      <td>154365.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13434.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>Blue</td>\n",
       "      <td>181577.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14043.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Make Colour  Odometer (KM)  Doors    Price\n",
       "0   Honda  White        35431.0    4.0  15323.0\n",
       "1     BMW   Blue       192714.0    5.0  19943.0\n",
       "2   Honda  White        84714.0    4.0  28343.0\n",
       "3  Toyota  White       154365.0    4.0  13434.0\n",
       "4  Nissan   Blue       181577.0    3.0  14043.0"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('zero-to-mastery-ml-master/data/car-sales-extended-missing-data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "92ec1986-5563-41e6-919e-055b967c88e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make              object\n",
       "Colour            object\n",
       "Odometer (KM)    float64\n",
       "Doors            float64\n",
       "Price            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "021821b2-0be8-491f-b8c1-8294b98f35a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             49\n",
       "Colour           50\n",
       "Odometer (KM)    50\n",
       "Doors            50\n",
       "Price            50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum() # Checando a quantidade de dados faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd218c2-ee13-4b32-91ff-2cf3f6957357",
   "metadata": {},
   "source": [
    "Etapas:\n",
    "1. Fill missing data\n",
    "2. Convert data to numbers\n",
    "3. Build a model on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c66e87dd-a10c-4ad1-80e8-e00c6d3ecc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22188417408787875"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Setup random seed\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import data and drop rows with missing labels\n",
    "data = pd.read_csv('zero-to-mastery-ml-master/data/car-sales-extended-missing-data.csv')\n",
    "data.dropna(subset=[\"Price\"], inplace=True)  # Retirando linhas que tem valores ausentes, mas nesse caso somente na coluna preço\n",
    "\n",
    "# Define different features and transformer pipeline\n",
    "categorical_features = [\"Make\", \"Colour\"]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "door_features = [\"Doors\"]\n",
    "door_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=4))\n",
    "])\n",
    "\n",
    "numeric_features = [\"Odometer (KM)\"]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\"))\n",
    "])\n",
    "\n",
    "# Setup preprocessing (fill missing values, then convert to numbers)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('door', door_transformer, door_features),\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Creating a preprocessing and modelling pipeline\n",
    "model = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", RandomForestRegressor())\n",
    "])\n",
    "\n",
    "# Split data\n",
    "X = data.drop(\"Price\", axis=1)\n",
    "y = data[\"Price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Fit and score the model\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff25355c-34c9-4e65-a1e1-04e97d262724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
